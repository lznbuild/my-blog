单项推送消息 server send event
制动重联，websocket手动重联
服务端返回的 Content-Type 是 text/event-stream，这是一个流，可以多次返回内容
```js

 const eventSource = new EventSource('http://localhost:3000/stream');
    eventSource.onmessage = ({ data }) => {
      console.log('New message', JSON.parse(data));
    };
```
这个 EventSource 是浏览器原生 api，就是用来获取 sse 接口的响应的，它会把每次消息传入 onmessage 的回调函数。

ie、edge 外，其他浏览器都没任何兼容
站内信
CI/CD 展示进度，日志的实时推送
chatGPT

websocket
首先通过 http 切换协议，服务端返回 101 的状态码后，就代表协议切换成功。

之后就是 WebSocket 格式数据的通信了，一方可以随时向另一方推送消息。

- ws 双向实时通信，传输文本、二进制数据等
- 仅支持服务器向客户端推送消息，客户端不能向服务器发送消息。
文本数据：只能传输文本数据，不支持二进制数据。自动重新连接



## http状态码
- 1XX   临时返回，表示客户端继续其请求

- 2XX 请求成功
  200 成功
  204 请求成功无返回内容


- 3XX   表示请求的目标有变化，希望客户端进一步处理


  301&302：永久性与临时性跳转。

  304：跟客户端缓存没有更新。产生这个状态的前提是：客户端本地已经有缓存的版本，并且在 Request 中告诉了服务端，当服务端通过时间或者 tag，发现没有更新的时候，就会返回一个不含 body 的 304 状态


- 4xx：客户端请求错误。
  400: 参数不对

  403：无权限。

  404：表示请求的资源不存在。

- 5xx：内部服务器错误




## GET POST请求的区别
- GET 参数通过 url 传递，POST 放在 body 中，POST相对安全

- GET 请求在 url 中传递的参数是有长度限制的，而 POST 没有，长度限制来源于浏览器对地址栏输入内容的限制，非GET请求本身的限制

- GET 在浏览器回退时是无害的（有缓存,协商缓存），而 POST 会再次提交请求,也就是说，GET 请求会被浏览器主动 cache，而 POST 不会，除非手动设置。

- GET 请求只能进行 url(x-www-form-urlencoded)编码，而 POST 支持多种编码方式。

- GET 产生一个 TCP 数据包；POST 产生两个 TCP 数据包。对于 GET 方式的请求，浏览器会把 http 的 header 和 data 一并发送出去，服务器响应200（返回数据）。而对于 POST，浏览器先发送 header，服务器响应100 continue，浏览器再发送 data，服务器响应200 ok（返回数据）



## POST和PUT请求的区别

PUT是幂等的，连续调用一次或者多次的效果相同（无副作用）, 如果两个请求相同，后一个请求会把第一个请求覆盖掉  （PUT更多的用来改资源）

POST是非幂等的方式，后一个请求不会把第一个请求覆盖掉（POST更多的用来增资源）


## http通用头部字段
- Cache-Control  控制缓存

- Connection 设置为Keep-alive用于告诉客户端本次HTTP请求结束之后并不需要关闭TCP连接，这样可以使下次HTTP请求使用相同的TCP通道，节省TCP连接建立的时间。

- Upgrade  升级为其他协议

- via 代理服务器的相关信息

- Transfor-Encoding 报文主体的传输编码格式

- Pragma 报文指令

- Date 创建报文的日期

- Content-Type


## HTTP请求头里有那些字段


- origin：发起请求的服务器地址

- Accept ：客户端或者代理能够处理的媒体类型

- Accept-Encoding 优先可处理的编码格式

- Accept-Language 优先可处理的自然语言

- Accept-Charset 优先可以处理的字符集

- If-Match 比较实体标记（ETage）

- If-None-Match 比较实体标记（ETage）与 If-Match相反

- If-Modified-Since 比较资源更新时间（Last-Modified)

- Authorization web的认证信息 ✨

- Host 请求资源所在服务器 ✨

- User-Agent 客户端程序信息 ✨

- Referer 请求原始放的url

- content-type 请求资源类型


## HTTP响应头里有那些字段

Location：令客户端重定向的URI (http code 302)

Last-Modified：该资源最后被修改的时间

set-cookie：设置cookie

content-length

Content-Type

date

ETag

Expires

keep-alive

（跨域有关）
  Access-Control-Allow-Headers
  Access-Control-Allow-Origin

## TCP为什么要三次握手
TCP是一种面向连接的单播协议，在发送数据前，通信双方必须在彼此间建立一条连接。所谓的“连接”，其实是客户端和服务器的内存里保存的一份关于对方的信息，如ip地址、端口号等

先大概说一下流程。

第一次握手，客户端发了个连接请求消息到服务端，服务端收到信息后知道自己与客户端是可以连接成功的，但此时客户端并不知道服务端是否已经接收到了它的请求。

第二次握手，服务端接收到消息后的应答，客户端得到服务端的反馈后，才确定自己与服务端是可以连接上的，
客户端只有确定了自己能与服务端连接上才能开始发数据。所以两次握手肯定是最基本的。

第三次握手，是为了防止已经失效的连接请求报文段突然又传到服务端，因而产生错误。如果没有第3次，服务器端是不知道客户端有没有接收到服务器端返回的信息，造成服务器端开销的严重浪费。

再归纳一下 每一步。



seq 序列号

ack 确认号，只有ACK为1时，它才有效， ack = seq+1

<!--
SYN=1 seq=x

ACK=1 SYN=1 seq=y ack=x+1

ACK=1 ack=y+1 seq=x+1
 -->
标志位

ACK: 确认序号有效
SYN: 发起一个新链接
FIN: 释放一个链接

第一次握手，客户端选择一个随机序列号seq（假设为x），并发送一个SYN数据包(1表示开启新连接，0结束连接)，其中可能还包括其他TCP标志和选项，客户端将这个数据包给服务器请求连接。

第二次握手， 服务器用一个带有确认应答ACK(1)和同步序列号SYN（1）,ack(x+1),seq(假设为y)的数据段，相应客户端，用来通知客户端，服务端已经收到SYN消息并通过了确认。

第三次握手，客户端笑了，说明，client发送给server，server发送给client都可以，但是server不知道这个整体流程走通了没，所以给ACK（1）seq(x+1),ack(y+1)发送握手期间的最后一个数据包，客户端收到数据后，发送数据段确认收到，服务器收到此数据确认无误，开始传送数据

其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据

### 第三次握手失败怎么办
第三次失败，只有客户端处于成功状态（因为第 2 次服务器返回了 ACK），服务器端没有接收到客户端的 ACK。超时重发，再失败，然后进入关闭状态


### 为什么要有同步序列号。

TCP协议可以是一个全双工通信的，接收方和发起方都可以发送信息，要有一个序列号，表示那些是同一个请求过程中的。序列号也不能从0开始，会混淆的，相对随机生成的，TCP规范中也不允许连接一直是一个半打开的状态。

## TCP的4次挥手
<!--
 FIN=1 seq=x
 ACK=1 ack=x+1 seq=v
 ACK=1 FIN=1 ack=x+1 seq=w
 ACK=1 ack=w+1 seq=x+1
 -->
1.首先客户端想要释放连接，向服务器端发送一段TCP报文（连接释放报文），FIN=1,seq=x并停止发送数据

2.服务器端接收到从客户端发出的TCP报文之后，确认了客户端想要释放连接，随后服务器端进入一个半关闭的状态（不能一直持续），并返回一段TCP报文（确认报文） ACK=1，ack=x+1,seq=v

3.服务器端准备好后，再次向客户端发出一段TCP报文  FIN =1,ACK=1, seq=w, ack=x+1 关闭。

4.客户端收到从服务器端发出的TCP报文，确认了服务器端已做好释放连接的准备，再次传输一段报文 ACK=1,ack=w+1,seq=x+1



## 为什么建立连接只通信了三次，而断开连接却用了四次？
因为TCP是全双工通信，在关闭连接的时候，既要关闭客户端的连接，也要关掉服务端建立的连接

TCP 是全双工模式，关闭连接时，当主机 B 收到主机 A 的 FIN 报文时，仅仅表示主机 A 不再发送数据了但是还能接收数据。此时，主机 B 也未必全部数据都发送给 A 了，所以 B 可以立即 close；也可以发送一些数据给 A 后，再发送 FIN 报文给对方来表示同意现在关闭连接，因此，主机 BACK 和 FIN 一般都会分开发送。


因为当服务端收到客户端的 FIN 报文后，发送的 ACK 报文只是用来应答的，并不表示服务端也希望立即关闭连接。

当只有服务端把所有的报文都发送完了，才会发送 FIN 报文，告诉客户端可以断开连接了，因此在断开连接时需要四次挥手。

TCP建立连接时之所以只需要"三次握手"，是因为在第二次"握手"过程中，服务器端发送给客户端的TCP报文是以SYN与ACK作为标志位的。SYN是请求连接标志，表示服务器端同意建立连接；ACK是确认报文，表示告诉客户端，服务器端收到了它的请求报文。

即SYN建立连接报文与ACK确认接收报文是在同一次"握手"当中传输的，所以"三次握手"不多也不少，正好让双方明确彼此信息互通。


所以是“三次握手”，“四次挥手”。


## DNS缓存查询过程
  搜索浏览器自身的DNS缓存==> 操作系统的hosts文件==> 查找本地DNS服务器 ==> 根服务器 ==> 顶级服务器

## options请求
 用于获取目的资源所支持的通信选项。
响应报文包含一个 Allow 首部字段，该字段的值表明了服务器支持的所有 HTTP 方法
使用 application/json 的 post 请求是必然会带入 OPTION 请求

在 CORS 中，可以使用 OPTIONS 方法发起一个预检请求，以检测实际请求是否可以被服务器所接受。预检请求报文中的 Access-Control-Request-Method 首部字段告知服务器实际请求所使用的 HTTP 方法；Access-Control-Request-Headers 首部字段告知服务器实际请求所携带的自定义首部字段。服务器基于从预检请求获得的信息来判断，是否接受接下来的实际请求。

## HTTP1.0   HTTP1.1   HTTP2.0 的区别

1.  HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法

2. HTTP1.1中新增了很多错误状态响应码

3. HTTP 1.1支持长连接。

    在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。通过设置http的请求头部和应答头部，保证本次数据请求结束之后，下一次请求仍可以重用这一通道，避免重新握手。原理是请求完成后不立即释放连接，而是放入连接池中，若这时有另一个请求要发出，请求的域名和端口是一样的，就直接拿出连接池中的连接进行发送和接收数据，少了建立连接的耗时。但有个问题，就是这个 keep-alive 的连接一次只能发送接收一个请求，在上一个请求处理完成之前，无法接受新的请求。

    若同时发起多个请求，就有两种情况：若串行发送请求，可以一直复用一个连接，但速度很慢，每个请求都要等待上个请求完成再进行发送。

    若并行发送这些请求，那么首次每个请求都要进行tcp三次握手建立新的连接，虽然第二次可以复用连接池里这堆连接，但若连接池里保持的连接过多，对服务端资源产生较大浪费，若限制了保持的连接数，并行请求里超出的连接仍每次要建连。

    对这个问题，新一代协议 HTTP2 提出了多路复用去解决。

4. HTTP/2 采用二进制格式传输数据，解析更高效

5. 多路复用，支持 TCP 连接复用
  通过引入二进制分帧层，就实现了 HTTP 的多路复用技术。
  ● 首先，浏览器准备好请求数据，包括了请求行、请求头等信息，如果是 POST 方法，那么还要有请求体
  ● 这些数据经过二进制分帧层处理之后，会被转换为一个个带有请求 ID 编号的帧，通过协议栈将这些帧发送给服务器。
  ● 服务器接收到所有帧之后，会将所有相同 ID 的帧合并为一条完整的请求信息。
  ● 然后服务器处理该条请求，并将处理的响应行、响应头和响应体分别发送至二进制分帧层。同样，二进制分帧层会将这些响应数据转换为一个个带有请求 ID 编号的帧，
  ● 经过协议栈发送给浏览器。浏览器接收到响应帧之后，会根据 ID 编号将帧的数据提交给对应的请求。

6. 加密。

    HTTPS 有两个作用，一是确定请求的目标服务端身份，二是保证传输的数据不会被网络中间节点窃听或者篡改。HTTPS 是使用加密通道来传输 HTTP 的内容。但是 HTTPS 首先与服务端建立一条 TLS 加密通道。TLS 构建于 TCP 协议之上，它实际上是对传输的内容做一次加密，所以从传输内容上看，HTTPS 跟 HTTP 没有任何区别。


7. HTTP2.0 支持服务端推送

    服务端推送能够在客户端发送第一个请求到服务端时，提前把一部分内容推送给客户端，放入缓存当中，这可以避免客户端请求顺序带来的并行度不高，从而导致的性能问题。
    服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。正因为没有发起请求，建立连接等操作，所以静态资源通过服务端推送的方式可以极大地提升速度。例如我的网页有一个sytle.css的请求，在客户端收到sytle.css数据的同时，服务端会将sytle.js的文件推送给客户端，当客户端再次尝试获取sytle.js时就可以直接从缓存中获取到，不用再发请求了。

    TCP 连接复用，则使用同一个 TCP 连接来传输多个 HTTP 请求，避免了 TCP 连接建立时的三次握手开销，和初建 TCP 连接时传输窗口小的问题。


## http1.1 中的长链接复用和HTTP2.0中的多路复用的区别
在HTTP/1.1协议中，浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制，超过限制数目的请求会被阻塞。(6个)所谓请求阻塞意思就是一条TCP的connection在同一时间只能允许一个请求经过，这样假如后续请求想要复用这个链接就必须等到前一个完成才行

HTTP/2多个请求可同时在一个连接上并行执行。某个请求任务耗时严重，不会影响到其它连接的正常执行,server端可以根据stream的唯一标识来相应对应的请求。HTTP2 的多路复用看起来是完美的解决方案，但还有个问题，就是队头阻塞，这是受限于 TCP 协议，TCP 协议为了保证数据的可靠性，若传输过程中一个 TCP 包丢失，会等待这个包重传后，才会处理后续的包。HTTP2的多路复用让所有请求都在同一条连接进行，中间有一个包丢失，就会阻塞等待重传，所有请求也就被阻塞了。这个问题无法解决，本身http协议就是基于TCP协议

事实上，在高丢包的环境下，HTTP/1.1 反而表现得更好，正是因为 HTTP/2 开了太多并行的 TCP 连接！

多路复用的技术可以只通过一个 TCP 连接就可以传输所有的请求数据

多路复用很好的解决了浏览器限制同一个域名下的请求数量的问题，同时也接更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度。

## HTTP的长连接有什么特点
http header中有个connection属性，close是短链接，connection：keep-alive是长连接，通过这个字段区分长连接和短链接。

优点：

1.减少TCP的握手次数

2.减少慢启动的影响  TCP可以发送很大的资源，100MB以上的这种，但网络带宽是有限的，所以就有一个优化机制，TCP连接都有一个由慢到快的过程，避免一下把带宽占满。

缺点：

tcp也是一种字符流的协议，传输一个文件，要从文件开头传到结尾，一个字节一个字节的传输，顺序不能乱，而且是串行的，会有队头阻塞问题


## TCP比较可靠？
TCP 之所以可靠，大体上由于以下原因：

数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时 TCP 发送数据端超时后会重发数据；

对失序数据包重排序：既然 TCP 报文段作为 IP 数据报来传输，而 IP 数据报的到达可能会失序，因此 TCP 报文段的到达也可能会失序。TCP 将对失序数据进行重新排序，然后才交给应用层；

丢弃重复数据：对于重复数据，能够丢弃重复数据；

应答机制：当 TCP 收到发自 TCP 连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒；

超时重发：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段；

流量控制：TCP 连接的每一方都有固定大小的缓冲空间。TCP 的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。





## TCP和UDP协议的区别
TCP : 可靠（重发机制），传递任意长度消息(字符流协议) 分包通过序列号组合，流量控制， 阻塞控制 ,需要建立连接，每一条TCP连接只能是1对1，提供全双工通信

UDP: 无连接，不可靠，面向报文，没有阻塞控制，一对一，一对多，多对一，多对多通讯，简单，快速，无队头阻塞的问题


## 全双工通信

单工数据传输只支持数据在一个方向上传输（电视）

半双工数据传输允许数据在两个方向上传输，但是，在某一时刻，只允许数据在一个方向上传输，它实际上是一种切换方向的单工通信；（对讲机）

全双工数据通信允许数据同时在两个方向上传输，因此，全双工通信是两个单工通信方式的结合，它要求发送设备和接收设备都有独立的接收和发送能力。（手机微信）



## 弱网环境下的前端优化手段有哪些

- DNS优化。由于DNS劫持或故障造成的服务不可用，进而影响用户体验。大部分标准DNS都是基于UDP与DNS服务器交互的，HTTPDNS则是利用HTTP协议与DNS服务器交互，绕开了运营商的Local DNS服务，有效防止了域名劫持，提高域名解析效率。简单来说就是DNS解析不通过运营商，通过自己的服务器

- 升级协议http2.0，多路复用。

- 数据压缩优化。 传输数据大小的问题。数据对请求速度的影响分两方面，一是压缩率，二是解压序列化反序列化的速度。目前最流行的两种数据格式是 json 和 protobuf，json 是字符串，protobuf 是二进制，即使用各种压缩算法压缩后，protobuf 仍会比 json 小，数据量上 protobuf 有优势，序列化速度 protobuf 也有一些优势
- PWA做离线缓存




## 正向代理

代理客户。需要主动设置代理服务器IP或者域名进行访问，由正向代理服务器去获取访问内容并返回。相当于中间人借钱，有钱人（服务器）只知道中间人（正向代理服务器）是谁，不知道真正借钱的人（客户端）是谁。隐藏真实的客户，为客户端收发请求，使真实客户端对服务器不可见。


实例：

一个局域网内的所有用户可能被一台服务器做了正向代理，由该台服务器负责 HTTP 请求，意味着同服务器做通信的是正向代理服务器;

## 反向代理

代理服务器。不需要客户端做任何设置，反向代理服务器会根据访问内容进行跳转及内容返回，你不知道它最终访问的是哪些机器。隐藏了真实的服务器，为服务器收发请求，使真实服务器对客户端不可见。相当于120电话，打120后不知道哪家医院的救护车会来。

实例：

负载均衡服务器，将用户的请求分发到空闲的服务器上;
意味着用户和负载均衡服务器直接通信，即用户解析服务器域名时得到的是负载均衡服务器的 IP。

## 正向代理和反向代理的共同点

- 都是做为服务器和客户端的中间层

- 都可以加强内网的安全性，阻止 web 攻击

- 都可以做缓存机制



## HTTP2.0
多路复用
支持传输2进制数据类型
服务器端主动推送
header压缩

## HTTP3.0
当然 HTTP/2 也并非完美，考虑一种情况，如果客户端或服务端在通信时出现数据包丢失，或者任何一方的网络出现中断，那么整个 TCP 连接就会暂停。

HTTP/2都是使用TCP协议来传输的，而如果使用HTTPS的话，还需要使用TLS协议进行安全传输，而使用TLS也需要一个握手过程，这样就需要有两个握手延迟过程,TCP的队头阻塞并没有彻底解决

读到这里，可能就会有人考虑为什么不直接去修改 TCP 协议？其实这已经是一件不可能完成的任务了。因为 TCP 存在的时间实在太长，已经充斥在各种设备中，并且这个协议是由操作系统实现的，更新起来不大现实。

HTTP/2 由于采用二进制分帧进行多路复用，通常只使用一个 TCP 连接进行传输，在丢包或网络中断的情况下后面的所有数据都被阻塞。但对于 HTTP/1.1 来说，可以开启多个 TCP 连接，任何一个 TCP 出现问题都不会影响其他 TCP 连接，剩余的 TCP 连接还可以正常传输数据。这种情况下 HTTP/2 的表现就不如 HTTP/1 了。

2018 年 HTTP/3 将底层依赖的 TCP 改成 UDP，谷歌搞了一个基于 UDP 协议的“QUIC”协议, http3跑在QUIC上，从而彻底解决对头阻塞这个问题。UDP 相对于 TCP 而言最大的特点是传输数据时不需要建立连接，可以同时发送多个数据包，所以传输效率很高，缺点就是没有确认机制来保证对方一定能收到数据。

QUIC：
- 实现了类似TCP的流量控制、传输可靠性的功能。
- 实现了快速握手功能。
- 集成了TLS加密功能
- 多路复用，彻底解决TCP中队头阻塞的问题


 ## 建立一个TCP连接后，是否会在一个HTTP请求完成断开？
 http1.0会，http1.1支持长链接，connection: keep-alive,维持TCP连接，SSL的开销也可以避免。


 ### 请求是异步的，为什么会造成阻塞
 http协议的队头阻塞，队头的任务没处理完，后面的都要等。

 http1.0 对于同一个TCP连接，所有的http请求放入队列，只有前一个请求的响应收到了，才能发下一个请求，阻塞在客户端。

 http1.1 允许发送多个请求，不必等前一个响应收到，客户端队头阻塞问题解决了，但是http1.1规定，服务器端的响应发送要根据请求被接收的顺序排队，先收到的请求的响应先发，阻塞了后面的响应发送，也会造成队头阻塞，发生在服务器端。

## 两个TCP建立请求相互之间同时发起时会发生什么？建立几个连接？
同时发起的两个请求最终只会建立一个连接

## content-type

text/html
text/plain
image/gif
image/jpeg


application/json
application/pdf
./octet-stream
./x-www-form-urlencoded  form 表单数据

multipart/form-data 表单中进行文件上传时，需要使用该格式


## TCP 弱网环境下的严重性能问题
- TCP 的拥塞控制算法会在丢包时主动降低吞吐量
- 3次握手增加开销

每一个TCP连接都会维护就一个拥塞控制窗口，有两个作用
- 防止发送方 向 接收方发送太多数据，导致接收方无法处理
- 防止TCP的任意方向向网络中发送大量数据，导致网络拥塞崩溃


### 为什么http1不能实现多路复用？

http1阶段是基于文本传输的，由于没有流的概念，在使用并行传输（多路复用）传递数据时，接收端在接收到响应后，并不能区分多个响应分别对应的请求，所以无法将多个响应的结果重新进行组装，也就实现不了多路复用。


### HTTP 1.0/1.1/2.0在并发请求上主要区别是什么?

1. HTTP/1.0
每次TCP连接只能发送一个请求，当服务器响应后就会关闭这次连接，下一个请求需要再次建立TCP连接.

2. HTTP/1.1
默认采用持续连接(TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive).
增加了管道机制，在同一个TCP连接里，允许多个请求同时发送，增加了并发性，进一步改善了HTTP协议的效率，
但是同一个TCP连接里，所有的数据通信是按次序进行的。回应慢，会有许多请求排队，造成"队头堵塞"。

3. HTTP/2.0

加了双工模式，即不仅客户端能够同时发送多个请求，服务端也能同时处理多个请求，解决了队头堵塞的问题。
使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。
增加服务器推送的功能，不经请求服务端主动向客户端发送数据。

### HTTP/1.1长连接和HTTP/2.0多路复用的区别?

HTTP/1.1：同一时间一个TCP连接只能处理一个请求, 采用一问一答的形式, 上一个请求响应后才能处理下一个请求. 由于浏览器最大TCP连接数的限制, 所以有了最大并发请求数的限制.
HTTP/2.0：同域名下所有通信都在单个连接上完成，消除了因多个 TCP 连接而带来的延时和内存消耗。单个连接上可以并行交错的请求和响应，之间互不干扰

chrome支持最大6个tcp连接

### 那为什么HTTP/1.1不能实现多路复用?

HTTP/2是基于二进制“帧”的协议，HTTP/1.1是基于“文本分割”解析的协议。

HTTP1.1的报文结构中, 服务器需要不断的读入字节，直到遇到换行符, 或者说一个空白行. 处理顺序是串行的, 一个请求和一个响应需要通过一问一答的形式才能对应起来.

```
GET / HTTP/1.1
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding:gzip, deflate, br
Accept-Language:zh-CN,zh;q=0.9,en;q=0.8
Cache-Control:max-age=0
Connection:keep-alive
Host:www.imooc.com
Referer:https://www.baidu.com/
```

HTTP2.0中，有两个非常重要的概念，分别是帧（frame）和流（stream）。
帧代表着最小的数据单位，每个帧会标识出该帧属于哪个流，流也就是多个帧组成的数据流。
多路复用，就是在一个 TCP 连接中可以存在多条流。换句话说，也就是可以发送多个请求，对端可以通过帧中的标识知道属于哪个请求。通过这个技术，可以避免 HTTP 旧版本中的队头阻塞问题，极大的提高传输性能。


### fetch 和 xhr 有什么区别
1. fetch的语法简洁，更语义化
2. 基于promise，支持async/await
3. 同构方便，使用isomorphic-fetch

Fetch的缺点：
1. fetch只对网络错误报错，http状态码错误不报错
2. fetch不支持abort，无法终止
3. fetch不支持超时控制，使用setTimeout和Promise.reject实现的超时控制不能阻止请求过程继续在后台运行，造成了流量的浪费
4. fetch没有原生检测请求进度的方式，XHR可以
5. 默认情况下fetch不发送cookie，除非手动配置


http1.0 1.1 2.0在并发请求上主要的区别是什么？

1.0 tcp连接只能发送一个请求

1.1 默认长链接 ， 管道机制，同一个tcp连接，允许多个请求同时发送，所有的数据通信是有顺序的，a,b,c ， a先到达服务器，必须有响应，才能处理b,c 对头阻塞


2.0
加了双工模式，服务器端也能同时处理多个请求，解决了对头阻塞的问题

多路复用 ， 没有次序的概念

tcp的滑动窗口
TCP 在收到数据包回复的 ACK 包里会带上自己接收窗口的大小，接收端需要根据这个值调整自己的发送策略。

TCP 会把要发送的数据放入发送缓冲区（Send Buffer)，接收到的数据放入接收缓冲区（Receive Buffer），应用程序会不停的读取接收缓冲区的内容进行处理。
流量控制做的事情就是，如果接收缓冲区已满，发送端应该停止发送数据。


websocket 和http的

相同点主要有：

都是基于TCP的应用层协议；
都使用Request/Response模型进行连接的建立；
在连接的建立过程中对错误的处理方式相同，在这个阶段WS可能返回和HTTP相同的返回码；


不同之处在于：

WS使用HTTP来建立连接，但是定义了一系列新的header域，这些域在HTTP中并不会使用；
WS连接建立之后，通信双方都可以在任何时刻向另一方发送数据；
WS连接建立之后，数据的传输使用帧来传递，不再需要Request消息；
WS的数据帧有序。

### tcp的粘包机制
指发送方发送的若干包数据到达接收方时粘成了一包，从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方。

 **出现原因**
tcp是基于字节流的协议， 基于流的传输不认为消息是一条一条的，是无保护消息边界的。并且为了将多个发往接收端的包，更有效的发到对方，发送方使用了优化方法（Nagle算法），将多次间隔较小、数据量小的数据，合并成一个大的数据块，然后进行封包。
接收方引起的粘包是由于接收方用户进程不及时接收数据，从而导致粘包现象。
TCP接收到数据包时，并不会马上交到应用层进行处理，将接收到的数据包保存在接收缓存里
如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。


如果发送方发送的多组数据本来就是同一块数据的不同部分，比如说一个文件被分成多个部分发送，这时当然不需要处理粘包现象。如果多个分组毫不相干，甚至是并列关系，那么这个时候就一定要处理粘包现象了

**解决方案**
发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。
靠设计一个带包头的应用层报文结构就能解决。包头定长，以特定标志开头，里带着负载长度，这样接收侧只要以定长尝试读取包头，再按照包头里的负载长度读取负载就行了

对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法，但有风险。

UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。


### 简单请求


get, head, post 无自定义头，content-type为
text/plain
multipart/form-data
application/x-www-form-urlencoded

为简单请求


put, delete 带自定义头 ，发送json数据，跨域场景下，
正式通信之前，非简单请求浏览器会先发送OPTION请求，进行预检，这一次的请求称为“预检请求”
服务器成功响应预检请求后，才会发送真正的请求，并且携带真实数据

https://mp.weixin.qq.com/s/ako-HuTP2rCjvr73GrCJfg
https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Overview
https://www.cnblogs.com/chenqf/p/6386163.html
https://juejin.im/post/5c7a9f8c518825640d1dd503#heading-16

https://mp.weixin.qq.com/s/2LJrvum4solO38bwL9tSDQ

https://zhuanlan.zhihu.com/p/339237818
